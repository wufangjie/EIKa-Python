* 概率论
** pdf, cdf, pmf
+ pdf: probability density function, 概率密度函数 (连续)
+ cdf: cumulative distribution function 累积分布函数
+ pmf: probability mass function 概率质量函数 (离散)

** conditional, joint, marginal
+ 监督学习的本质就是求条件概率 P(Y|X)
+ 联合分布: 同时发生的概率, 特别的, 对于独立分布有 P(X, Y) = P(X) * P(Y), 再进一步就是似然函数
+ 边缘分布: 联合分布在别的维度上叠加 (积分, 求和)

** 样本方差和总体方差
+ 如果给出的数据就是整体, 那么计算方差时应除以 n
+ 如果给出的是样本, 那估计整体方差时应除以 n - 1

** bayes 贝叶斯公式和朴素贝叶斯
P(Y|X) = P(Y, X) / P(X) = P(Yi) * P(X | Yi) / \Sum P(Yi) * P(X | Yi)

* 统计学
** likelihood
+ 似然函数: 假设样本是基于某一分布产生的, 那么这些样本出现的联合概率就是似然函数, 特别的, 样本的产生相互独立, 可以写成连乘形式
+ 极大似然估计: 求使得似然函数最大的(模型, 分布)参数
+ 对数似然: (连续无间断参数情形) 取对数, 化乘法为加法, 这时可以方便地通过求偏导来算极值

** R-Squared
R2: 1 - SSE/SST
SSE: 预测的残差平方和, (weight * (y_true - y_pred) ** 2).sum()
SST: (weight * (y_true - y_true_weighted_avg)) ** 2).sum() (NOTE: 真实值的加权平均)
Adjusted R2: 1 - (1 - R2) * (n - 1) / (n - k - 1), 其中 n 是数据条数, k 是模型用到的指标个数 (一种正则?)

** 假设检验 (Hypothesis Testing)
+ 例子: 有一批货, 据说次品率 <= 5%, 如何检验它的真伪?
+ 假设检验: 设计检验统计量, 确定拒绝域, 做抽样, 计算检验统计量检验 (null hypothesis)
+ 直观描述: 抽样 5 个, 确定 k (检验统计量), 当抽样次品数 > k (拒绝域)时, 拒绝接受 <= 5% 的假设, 否则无法拒绝 (但也不是说它就是真的)
+ 问题1: 如何科学地设置 k 这个值?
+ 直观描述: 为了方便计算取 m = 5, 即进行 m 次独立试验, 整个分布满足二项分布, 计算 p = 5% 时出现 x 个次品的概率:
| x    |       0 |       1 |       2 |       3 |       4 | 5 |
| p=5% | 0.77378 | 0.20363 | 0.02143 | 0.00113 | 0.00003 | 0 |
可以看出, 当 p <= 5% 时, 最多出现一个次品的概率是 0.97741, 直觉上我们已经可以作出判断, 当这次抽样出现大于一个次品时, p <= 5% 这个假设大概率是假的
+ 问题2: 那么如何刻画我们对这次判断的信心呢?
+ 犯第一类错误(即假设为真但拒绝)的概率: 上述例子是 <= 0.02259
+ 犯第二类错误(即假设为假但不拒绝)的概率: 上述例子是 < 0.97741 (注意与第一类错误的关系, 但两者不会同时发生)
+ alpha (检验水平, 显著性水平, significant level): 设定的值, 通常取 0.01, 0.05, 0.1, 用来要求犯第一类错误的小于该值 (控制第一类错误概率原则)
+ 如果要求 alpha = 0.05, 那么 k = 1 显然是个好选择, 如果希望犯第一类错误的概率要更小, 比如 alpha = 0.01, 那么此时 k 至少取 2, 也就是越宽松

** p-value
+ 问题: 接着上述例子, 在 m=5, k=1 抽样中, 如何评价出现了两次次品和三次次品的区别?
+ p-value: 能拒绝假设的最小显著性水平, 上述例子分别是 0.00116 和 0.00003
+ 最小指的是, 如果更小的话, 就不能拒绝原假设了
+ p-value: 越小, (根据假设)越小概率的事情发生了, 我们就越有理由拒绝原假设
+ 特征选择时的 p-value: 一般是假设特征与目标无关, 然后根据样本计算出 p-value

* 信息论 (TODO)
** 信息熵 (entropy of an information source)
其最大值的证明, 用 jensen 不等式是不对的, 应用 k 因子法, 即在加上一项 k(\Simga{}p_{i} - 1), 然后求对 p_{i} 无限制的整个式子的极大值, 显然对每个 p_{i} 要求 k 的值是一样的, 同时我们令 k 等于正好使得在取得该极值时的 \Simga{}p_{i} = 1 即可. 这个方法也称为拉格朗日乘子法, 其实我理解的是对于一般的问题加了这一项也未必能算出极值(注: 很多 EM 算法会用到这个)

H(p) = -\Simga_{x} p(x) * log(p(x)) # NOTE: 信息论的 log 都是指 log2, 其实是其他也无所谓, 就是差一个常数

针对的是一个分布，该分布越混乱（均匀），信息熵越大

** 联合熵 (joint entropy)
H(X, Y) = -\Simga_{x} \Simga_{y} p(x, y) * log(p(x, y))

** 条件熵 (conditional entropy)
H(X|Y) = -\Sigma_{y} p(y) * H(X|Y = y) = -\Sigma_{y} \Sigma_{x} p(y) * p(x|y) * log(p(x|y)) = -\Sigma_{y} \Sigma_{x} p(x, y) * log(p(x|y))

** 互信息 (mutual information)
I(X, Y) = \Simga_{x} \Simga_{y} p(x, y) * log(p(x, y) / p(x) / p(y))

** 上述几个的关系
*** 可以把 H(X), H(Y) 可以看成两个任意集合

*** 集合的交就是互信息 I(X, Y)
I(X, Y) = \Simga_{x} \Simga_{y} p(x, y) * log(p(x|y) / p(x)) = \Simga_{x} \Simga_{y} p(x, y) * (log(p(x|y)) - log(p(x))) = -H(X|Y) + H(X)

*** 集合的并就是联合熵
因为不能直接做集合的并, 所以只能先化成不相交的两部分再相加
H(X, Y) = -\Simga_{x} \Simga_{y} p(x, y) * log(p(x, y)) = -\Simga_{x} \Simga_{y} p(x, y) * log(p(y) * p(x|y)) = H(Y) + H(X|Y)

*** 集合的差就是条件熵
因为不能直接做集合的差, 所以只能先化成有包含关系的两部分再相减 (互信息)
H(X|Y) = -\Sigma_{x} \Sigma_{y} p(x, y) * log(p(x|y) * p(y) / p(y)) = H(X, Y) - H(Y)

** 交叉熵 (cross entropy)
H(p, q) = -\Sigma_{x} p(x) * log(q(x))
p 为真实分布, q 为预测分布, H(p, q) 可表示两者的相似性

** KL散度 (Kullback-Leibler divergence)
D_{KL}(p||q) = -\Sigma_{x} p(x) * log(q(x) / p(x))
D_{KL}(p||q) = H(p, q) - H(p) 和交叉熵就差一个"常数", 所以也称为相对熵
非对称, 可以改进为 (D_{KL}(p||q) + D_{KL}(q||p)) / 2
非负性, 同样可利用拉格朗日乘子法, k = 1 时等号成立

** 总结
公式都有负号
中间三个都是以 p(x, y) 为权重求某个 log 的期望
最后两个都是以真实分布 p(x) 为权重求关于新分布相关 log 的期望

* pipeline
一系列实现 (fit, transform, fit_transform) 函数的对象, 最后一个只需要 fit
#+BEGIN_SRC python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.svm import SVC

X, y = make_classification(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
pipe = Pipeline([('scaler', StandardScaler()),
                 ('select', SelectKBest(k=7)),
                 ('svc', SVC())])
pipe.fit(X_train, y_train)
print(pipe.score(X_test, y_test))
#+END_SRC

** preprocessing
cols = []
*** scaler
#+BEGIN_SRC python
from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler

scaler = StandardScaler()
scaler.fit(X) # scaler.fit_transform(X)
scaler.transform(X2)
#+END_SRC

*** Binarizer
*** encoder
#+BEGIN_SRC python
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder

col = np.array([c for c in 'congratulation']).reshape(-1, 1)
enc = OneHotEncoder()
#+END_SRC

*** impute
#+BEGIN_SRC python
from sklearn.impute import SimpleImputer
help(SimpleImputer)
#+END_SRC

* feature_selection (三类方法)
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限

#+BEGIN_SRC python
from sklearn.feature_selection import SelectFromModel, SelectKBest

SelectFromModel(LogisticRegression(penalty="l1", C=0.1))
#+END_SRC

** 过滤法 (Filter), 根据统计量, 删除不好的特征 (pearson, 方差小的, chi2)
+ r_regression (Pearson 相关系数, 简单直接, 但只能用于线性关系)
+ f_regression Univariate linear regression tests returning F-statistic and p-values (回归分析)
+ f_classif ANOVA F-value between label/feature for classification tasks.
+ chi2 Chi-squared stats of non-negative features for classification tasks.
+ mutual_info TODO:

** 包装法 (Wrapper)
+ 逐步回归
+ 递归消除特征 (RFE), 用模型迭代删除 feature importance 低的特征 (或选出高的特征子集)
** 嵌入法 (Embedded), 通过 L1 正则的稀疏性约束达到筛选特征的目的

* model_selection + metrics
** model selection
#+BEGIN_SRC python
from sklearn.model_selection import (train_test_split,
                                     GridSearchCV,
                                     cross_val_score,
                                     learning_curve)

help(train_test_split)

# ...

temp = cross_val_score(mod, X, y, **cv_params) # shape[1] == nfold
print((np.mean(temp), np.std(temp)))

train_sizes, train_scores, test_scores = learning_curve(
    mod, X, y, train_sizes=train_sizes, **cv_params)
# train_sizes 看出不同样本数量对学习曲线的影响, 是否过/欠拟合
#+END_SRC

** confusion matrix (混淆矩阵)
| TP       | FP(假阳) |
| FN(假阴) | TN       |

** accuracy (准确率)
(TP + TN) / ALL

** precision (精确率, 查准率)
TP / (TP + FP)
查出来的查对的概率, 应用于尽量不要误判的情况, 比如垃圾邮件过滤

** recall (召回率, 查全率)
TP / (TP + FN)
能查出来的概率, 尽量用于发现问题, 后续人为跟进的场景, 比如疾病诊断

** F1
precision, recall 的调和平均

** ROC
TPR(召回率)(sensitivity): 实际有病, 被检测出来的百分比
FPR(假阳性率)(1 - specificity): FP / (FP + TN) 实际没病, 检测出有病的百分比
ROC: TPR-FPR 曲线 (y-x 轴)

几点解释:
+ 随机猜测的话, 差不多就是一条 y = x 的直线
+ 全预测 1 的话, 召回率就是 1, 假阳性率也是 1, 就是点 (1, 1)
+ 全预测 0 的话, 召回率就是 0, 假阳性率也是 0, 就是点 (0, 0)
+ 完美预测的话, 就是点 (0, 1), 所以曲线上凸越明显越好
+ 模型的 TPR, FPR 是确定的, 也就是说只是曲线上的一点, 那如何得到曲线呢? 一般二分类模型都会预测出一个概率, 我们可以通过调整这个概率阈值(0->1)生成曲线
+ 相比 precision-recall (P-R) 曲线, ROC 有一个巨大优势: 当正负样本分布变化时, 其形状能基本保持不变
+ 两种最优临界点(最优阈值): 1. 距 (0, 1) 最近; 2. 与 y = x 的垂直距离最大 (Youden index)

** AUC
area under curve, 也就是 ROC曲线下方的面积

** code example
#+BEGIN_SRC python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
idx = np.random.rand(len(y)) > 0.5

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=0)
clf.fit(X[idx], y[idx])

y_prob = clf.predict_proba(X[~idx])[:, 1]
y_true = y[~idx] == 1

precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)
plt.plot(recalls, precisions, 'x-')

fpr, tpr, thresholds = roc_curve(y_true, y_prob)
plt.plot(fpr, tpr)
print(roc_auc_score(y_true, y_prob))
#+END_SRC

** 回归度量, 自定义度量
#+BEGIN_SRC python
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.metrics import maker_scorer
help(make_scorer)
#+END_SRC

* 其他
** regularization
** ensemble
bagging: 抽样训练
boosting: 拟合残差

** 其他
PCA 降维
kmeans 聚类
层次聚类
DBSCAN (密度)

** EM
*** lagrange multipler
约束条件移项并引入一个自由参数成为一项, 因为极值时, 对该自由参数的偏导数为 0, 即约束条件成立

有一些文章用 jensen's inequality 进行证明是不对的一些极值, 应该用拉格朗日乘子
我的理解是求有等式约束条件的极值, 直接对每个参数求偏导是不对的 (结果不满足约束条件), 比如 \Sigma(pi) = 1, 那么再引入一个参数乘以这个移项后等于 0 的约束条件, 可以达到代入约束条件, 减少参数, 然后求偏导一样的效果
有多个约束, 但是对于不同变量的, 每次只加一项, 详见 HMM 对 aij 的估计
看了 wiki, 发现这只是一点皮毛, 不过够用了

** kernel method
把低维空间线性不可分的数据通过核函数投射到高维空间实现线性分割

* 其他名词
** 皮尔逊相关性 (pearson correlation)
p(X, Y) = cov(X, Y) / \delta(X) / \delta(Y)

** 余弦距离(cosine similarity)
cos(\theta) = (X, Y) / (X, X)^0.5 / (Y, Y)^0.5

** IoU (intersection over union) / jaccard index / Tanimoto
用于 image detection, 稀疏向量的相似性 (非零元素的集合)

** 损失函数和风险函数
+ 0-1 损失函数
+ 平方损失函数
+ 绝对损失函数
+ 对数损失函数（极大似然估计，使得联合概率最大参数）
+ 交叉熵

+ 指数损失
+ Hinge
+ 感知损失

** 经验风险最小化和结构风险最小化
